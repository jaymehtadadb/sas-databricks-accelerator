# sas-databricks-accelerator
A set of notebooks to accelerate converting SAS code to Pyspark and SparkSQL for business users

In this accelerator, you'll learn the fundamentals of Databricks while converting SAS code to Databricks. As a SAS user, you're likely familiar with the power and flexibility of SAS programming. However, with the increasing adoption of cloud-based big data analytics, it's essential to learn how to work with Databricks, a fast, scalable, and collaborative analytics platform. In this module, we'll guide you through the process of converting SAS code to Databricks, using real-world examples and hands-on exercises

## Scope of the Accelerator
This module focuses on the concepts that are essential to convert your SAS code to Databricks and assumes that the required data sets are already migrated over. Therefore, this module only covers the code translation and does not include ingestion of SAS data files in the Lakehouse. Databricks file upload utility allows you to use the UI to create a Delta table by importing small CSV, TSV, JSON, Avro, Parquet, or text files from your local machine.

## Prerequisites
Understanding of Databricks Lakehouse platform and Delta Lake
Beginner experience using SQL to query data from enterprise data stores. Review Data Analysis With Databricks SQL course on Databricks Academy
Beginner programming experience with Python. Review Just Enough Python for Apache Spark course on Databricks Academy

## Technical Considerations
Use a single-node cluster to try out hands-on. This course runs on DBR 14.3+
This course works on Databricks Community Edition
Create table access is not required as we create temporary views throughout this course
